"""
IMDB Sentiment Analysis Project - Jupyter Notebook Version
Optimized for Jupyter with inline visualizations

Dataset: IMDB Dataset of 50K Movie Reviews
Source: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
import os
warnings.filterwarnings('ignore')

# For Jupyter inline plots
%matplotlib inline

# Text Processing
import re
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
import nltk

# Feature Engineering
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD

# Machine Learning Models
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier

# Metrics
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, confusion_matrix, classification_report,
                             roc_auc_score, roc_curve)

# Create output directory
OUTPUT_DIR = 'output_visualizations'
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
    print(f"✓ Created output directory: {OUTPUT_DIR}")

# Download required NLTK data
print("Downloading NLTK data...")
for item in ['stopwords', 'punkt', 'wordnet', 'averaged_perceptron_tagger']:
    try:
        nltk.download(item, quiet=True)
    except:
        pass

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Set figure size for better viewing
plt.rcParams['figure.figsize'] = (12, 8)

# ============================================================================
# SECTION 1: DATA LOADING AND INITIAL EXPLORATION
# ============================================================================

def load_data(filepath='IMDB Dataset.csv'):
    """Load the IMDB dataset"""
    print("\n" + "="*80)
    print("LOADING DATASET")
    print("="*80)
    
    try:
        df = pd.read_csv(filepath)
        print(f"✓ Dataset loaded successfully!")
        print(f"  Shape: {df.shape}")
        print(f"  Columns: {list(df.columns)}")
        return df
    except FileNotFoundError:
        print("ERROR: Dataset not found!")
        print("\nPlease download the dataset from:")
        print("https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews")
        print("\nAnd place 'IMDB Dataset.csv' in the current directory.")
        return None

# ============================================================================
# SECTION 2: EXPLORATORY DATA ANALYSIS (EDA)
# ============================================================================

def perform_eda(df):
    """Comprehensive Exploratory Data Analysis"""
    print("\n" + "="*80)
    print("EXPLORATORY DATA ANALYSIS (EDA)")
    print("="*80)
    
    # Basic information
    print("\n1. DATASET OVERVIEW")
    print("-" * 40)
    print(f"Number of reviews: {len(df)}")
    print(f"Number of features: {df.shape[1]}")
    print(f"\nFirst few rows:")
    print(df.head(3))
    
    # Data types and missing values
    print("\n2. DATA QUALITY CHECK")
    print("-" * 40)
    print(f"\nData types:\n{df.dtypes}")
    print(f"\nMissing values:\n{df.isnull().sum()}")
    print(f"\nDuplicate rows: {df.duplicated().sum()}")
    
    # Sentiment distribution
    print("\n3. SENTIMENT DISTRIBUTION")
    print("-" * 40)
    sentiment_counts = df['sentiment'].value_counts()
    print(sentiment_counts)
    print(f"\nPercentage distribution:")
    print(df['sentiment'].value_counts(normalize=True) * 100)
    
    # Review length statistics
    print("\n4. REVIEW LENGTH ANALYSIS")
    print("-" * 40)
    df['review_length'] = df['review'].apply(len)
    df['word_count'] = df['review'].apply(lambda x: len(x.split()))
    
    print(f"\nCharacter length statistics:")
    print(df['review_length'].describe())
    print(f"\nWord count statistics:")
    print(df['word_count'].describe())
    
    # Sentiment-wise statistics
    print(f"\nAverage word count by sentiment:")
    print(df.groupby('sentiment')['word_count'].mean())
    
    return df

def visualize_eda(df):
    """Create comprehensive EDA visualizations - Shows in Jupyter"""
    print("\n5. GENERATING EDA VISUALIZATIONS")
    print("-" * 40)
    
    fig = plt.figure(figsize=(20, 12))
    
    # 1. Sentiment distribution
    plt.subplot(2, 3, 1)
    sentiment_counts = df['sentiment'].value_counts()
    plt.bar(sentiment_counts.index, sentiment_counts.values, color=['#ff6b6b', '#4ecdc4'])
    plt.title('Sentiment Distribution', fontsize=14, fontweight='bold')
    plt.xlabel('Sentiment')
    plt.ylabel('Count')
    for i, v in enumerate(sentiment_counts.values):
        plt.text(i, v + 500, str(v), ha='center', va='bottom', fontweight='bold')
    
    # 2. Review length distribution
    plt.subplot(2, 3, 2)
    plt.hist(df['review_length'], bins=50, color='#95e1d3', edgecolor='black', alpha=0.7)
    plt.axvline(df['review_length'].mean(), color='red', linestyle='--', 
                label=f'Mean: {df["review_length"].mean():.0f}')
    plt.title('Distribution of Review Length (Characters)', fontsize=14, fontweight='bold')
    plt.xlabel('Length (characters)')
    plt.ylabel('Frequency')
    plt.legend()
    
    # 3. Word count distribution
    plt.subplot(2, 3, 3)
    plt.hist(df['word_count'], bins=50, color='#f38181', edgecolor='black', alpha=0.7)
    plt.axvline(df['word_count'].mean(), color='blue', linestyle='--', 
                label=f'Mean: {df["word_count"].mean():.0f}')
    plt.title('Distribution of Word Count', fontsize=14, fontweight='bold')
    plt.xlabel('Word count')
    plt.ylabel('Frequency')
    plt.legend()
    
    # 4. Box plot - word count by sentiment
    plt.subplot(2, 3, 4)
    df.boxplot(column='word_count', by='sentiment', ax=plt.gca())
    plt.title('Word Count Distribution by Sentiment', fontsize=14, fontweight='bold')
    plt.suptitle('')
    plt.xlabel('Sentiment')
    plt.ylabel('Word Count')
    
    # 5. Review length by sentiment
    plt.subplot(2, 3, 5)
    for sentiment in df['sentiment'].unique():
        data = df[df['sentiment'] == sentiment]['review_length']
        plt.hist(data, bins=30, alpha=0.5, label=sentiment)
    plt.title('Review Length by Sentiment', fontsize=14, fontweight='bold')
    plt.xlabel('Length (characters)')
    plt.ylabel('Frequency')
    plt.legend()
    
    # 6. Violin plot - word count by sentiment
    plt.subplot(2, 3, 6)
    sentiment_order = df['sentiment'].unique()
    sns.violinplot(data=df, x='sentiment', y='word_count', order=sentiment_order)
    plt.title('Word Count Distribution (Violin Plot)', fontsize=14, fontweight='bold')
    plt.xlabel('Sentiment')
    plt.ylabel('Word Count')
    
    plt.tight_layout()
    
    # Save and show
    output_path = os.path.join(OUTPUT_DIR, 'eda_visualizations.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"✓ Saved to: {output_path}")
    plt.show()  # Display in Jupyter
    print("✓ EDA visualizations displayed above")

# ============================================================================
# SECTION 3: TEXT PREPROCESSING & DATA MINING TECHNIQUES
# ============================================================================

class TextPreprocessor:
    """Comprehensive text preprocessing with multiple techniques"""
    
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()
        self.stemmer = PorterStemmer()
        try:
            self.stop_words = set(stopwords.words('english'))
        except:
            self.stop_words = set()
    
    def remove_html_tags(self, text):
        """Remove HTML tags"""
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text)
    
    def remove_urls(self, text):
        """Remove URLs"""
        return re.sub(r'http\S+|www.\S+', '', text)
    
    def remove_punctuation(self, text):
        """Remove punctuation"""
        return text.translate(str.maketrans('', '', string.punctuation))
    
    def remove_numbers(self, text):
        """Remove numbers"""
        return re.sub(r'\d+', '', text)
    
    def to_lowercase(self, text):
        """Convert to lowercase"""
        return text.lower()
    
    def remove_extra_whitespace(self, text):
        """Remove extra whitespace"""
        return ' '.join(text.split())
    
    def remove_stopwords(self, text):
        """Remove stopwords"""
        words = text.split()
        return ' '.join([word for word in words if word not in self.stop_words])
    
    def lemmatize(self, text):
        """Lemmatize words"""
        words = text.split()
        return ' '.join([self.lemmatizer.lemmatize(word) for word in words])
    
    def stem(self, text):
        """Stem words"""
        words = text.split()
        return ' '.join([self.stemmer.stem(word) for word in words])
    
    def preprocess(self, text, use_stemming=False):
        """Complete preprocessing pipeline"""
        # Remove HTML and URLs
        text = self.remove_html_tags(text)
        text = self.remove_urls(text)
        
        # Convert to lowercase
        text = self.to_lowercase(text)
        
        # Remove punctuation and numbers
        text = self.remove_punctuation(text)
        text = self.remove_numbers(text)
        
        # Remove extra whitespace
        text = self.remove_extra_whitespace(text)
        
        # Remove stopwords
        text = self.remove_stopwords(text)
        
        # Lemmatize or stem
        if use_stemming:
            text = self.stem(text)
        else:
            text = self.lemmatize(text)
        
        return text

def preprocess_data(df, sample_size=None):
    """Preprocess the dataset"""
    print("\n" + "="*80)
    print("TEXT PREPROCESSING")
    print("="*80)
    
    # Sample data if needed (for faster processing)
    if sample_size and sample_size < len(df):
        df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)
        print(f"Using sample of {sample_size} reviews for faster processing")
    
    preprocessor = TextPreprocessor()
    
    print("\nApplying preprocessing pipeline...")
    print("- Removing HTML tags")
    print("- Converting to lowercase")
    print("- Removing punctuation and numbers")
    print("- Removing stopwords")
    print("- Lemmatizing words")
    
    df['cleaned_review'] = df['review'].apply(lambda x: preprocessor.preprocess(x))
    
    print(f"\n✓ Preprocessing complete!")
    print(f"\nExample transformation:")
    print(f"Original: {df['review'].iloc[0][:200]}...")
    print(f"\nCleaned: {df['cleaned_review'].iloc[0][:200]}...")
    
    return df

# ============================================================================
# SECTION 4: FEATURE ENGINEERING & DATA MINING
# ============================================================================

def extract_text_features(df):
    """Extract additional text features (Data Mining)"""
    print("\n" + "="*80)
    print("FEATURE ENGINEERING - DATA MINING TECHNIQUES")
    print("="*80)
    
    print("\n1. Extracting statistical features...")
    
    # Length features
    df['char_count'] = df['cleaned_review'].apply(len)
    df['word_count'] = df['cleaned_review'].apply(lambda x: len(x.split()))
    df['avg_word_length'] = df['cleaned_review'].apply(
        lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0
    )
    
    # Sentiment-indicative features
    df['exclamation_count'] = df['review'].apply(lambda x: x.count('!'))
    df['question_count'] = df['review'].apply(lambda x: x.count('?'))
    df['uppercase_count'] = df['review'].apply(lambda x: sum(1 for c in x if c.isupper()))
    
    print("✓ Statistical features extracted")
    
    return df

def analyze_word_frequency(df):
    """Analyze most common words (Association Rule Mining concept)"""
    print("\n2. Word Frequency Analysis (Association Mining)...")
    
    # Combine all reviews
    all_words_positive = ' '.join(df[df['sentiment'] == 'positive']['cleaned_review']).split()
    all_words_negative = ' '.join(df[df['sentiment'] == 'negative']['cleaned_review']).split()
    
    # Count frequencies
    positive_freq = Counter(all_words_positive).most_common(20)
    negative_freq = Counter(all_words_negative).most_common(20)
    
    print("\nTop 10 words in POSITIVE reviews:")
    for word, count in positive_freq[:10]:
        print(f"  {word}: {count}")
    
    print("\nTop 10 words in NEGATIVE reviews:")
    for word, count in negative_freq[:10]:
        print(f"  {word}: {count}")
    
    # Visualize
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    words_pos, counts_pos = zip(*positive_freq)
    ax1.barh(words_pos, counts_pos, color='#4ecdc4')
    ax1.set_xlabel('Frequency')
    ax1.set_title('Top 20 Words in Positive Reviews', fontweight='bold', fontsize=14)
    ax1.invert_yaxis()
    
    words_neg, counts_neg = zip(*negative_freq)
    ax2.barh(words_neg, counts_neg, color='#ff6b6b')
    ax2.set_xlabel('Frequency')
    ax2.set_title('Top 20 Words in Negative Reviews', fontweight='bold', fontsize=14)
    ax2.invert_yaxis()
    
    plt.tight_layout()
    
    # Save and show
    output_path = os.path.join(OUTPUT_DIR, 'word_frequency.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\n✓ Saved to: {output_path}")
    plt.show()  # Display in Jupyter
    print("✓ Word frequency visualization displayed above")
    
    return positive_freq, negative_freq

def create_word_clouds(df):
    """Create word clouds for visualization"""
    try:
        from wordcloud import WordCloud
        
        print("\n3. Generating Word Clouds...")
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
        
        # Positive reviews word cloud
        positive_text = ' '.join(df[df['sentiment'] == 'positive']['cleaned_review'])
        wordcloud_pos = WordCloud(width=800, height=400, background_color='white',
                                   colormap='Greens').generate(positive_text)
        ax1.imshow(wordcloud_pos, interpolation='bilinear')
        ax1.axis('off')
        ax1.set_title('Positive Reviews - Word Cloud', fontweight='bold', fontsize=16)
        
        # Negative reviews word cloud
        negative_text = ' '.join(df[df['sentiment'] == 'negative']['cleaned_review'])
        wordcloud_neg = WordCloud(width=800, height=400, background_color='white',
                                   colormap='Reds').generate(negative_text)
        ax2.imshow(wordcloud_neg, interpolation='bilinear')
        ax2.axis('off')
        ax2.set_title('Negative Reviews - Word Cloud', fontweight='bold', fontsize=16)
        
        plt.tight_layout()
        
        # Save and show
        output_path = os.path.join(OUTPUT_DIR, 'wordclouds.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"✓ Saved to: {output_path}")
        plt.show()  # Display in Jupyter
        print("✓ Word clouds displayed above")
    except ImportError:
        print("⚠ WordCloud library not available. Install with: pip install wordcloud")

# ============================================================================
# SECTION 5: MACHINE LEARNING MODELS
# ============================================================================

def prepare_features(df, max_features=5000):
    """Prepare features using TF-IDF (Data Mining technique)"""
    print("\n" + "="*80)
    print("FEATURE EXTRACTION - TF-IDF VECTORIZATION")
    print("="*80)
    
    # Encode labels
    df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})
    
    # TF-IDF Vectorization
    print(f"\nCreating TF-IDF features (max_features={max_features})...")
    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(1, 2))
    X_tfidf = tfidf.fit_transform(df['cleaned_review'])
    
    print(f"✓ TF-IDF matrix shape: {X_tfidf.shape}")
    
    # Split data
    y = df['label']
    X_train, X_test, y_train, y_test = train_test_split(
        X_tfidf, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\nTrain set: {X_train.shape[0]} samples")
    print(f"Test set: {X_test.shape[0]} samples")
    
    # Return only 4 values (FIXED)
    return X_train, X_test, y_train, y_test

def train_traditional_models(X_train, X_test, y_train, y_test):
    """Train multiple traditional ML models"""
    print("\n" + "="*80)
    print("TRAINING MACHINE LEARNING MODELS")
    print("="*80)
    
    models = {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'Naive Bayes': MultinomialNB(),
        'Linear SVM': LinearSVC(max_iter=1000, random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Decision Tree': DecisionTreeClassifier(random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\n{name}")
        print("-" * 40)
        print("Training...")
        
        # Train
        model.fit(X_train, y_train)
        
        # Predict
        y_pred = model.predict(X_test)
        
        # Evaluate
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        
        results[name] = {
            'model': model,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'predictions': y_pred
        }
        
        print(f"Accuracy:  {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall:    {recall:.4f}")
        print(f"F1-Score:  {f1:.4f}")
    
    return results

def visualize_model_comparison(results):
    """Visualize model performance comparison"""
    print("\n" + "="*80)
    print("MODEL PERFORMANCE COMPARISON")
    print("="*80)
    
    # Prepare data
    models = list(results.keys())
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.ravel()
    
    for idx, metric in enumerate(metrics):
        values = [results[model][metric] for model in models]
        
        ax = axes[idx]
        bars = ax.bar(models, values, color=['#667eea', '#764ba2', '#f093fb', '#4facfe', '#00f2fe', '#43e97b'])
        ax.set_title(f'{metric.capitalize()} Comparison', fontweight='bold', fontsize=14)
        ax.set_ylabel(metric.capitalize())
        ax.set_ylim([0, 1.0])
        ax.tick_params(axis='x', rotation=45)
        
        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.4f}',
                   ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    
    # Save and show
    output_path = os.path.join(OUTPUT_DIR, 'model_comparison.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"✓ Saved to: {output_path}")
    plt.show()  # Display in Jupyter
    print("✓ Model comparison displayed above")
    
    # Print summary table
    print("\nPerformance Summary:")
    print("-" * 80)
    print(f"{'Model':<25} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}")
    print("-" * 80)
    for model_name, metrics in results.items():
        print(f"{model_name:<25} {metrics['accuracy']:<12.4f} {metrics['precision']:<12.4f} "
              f"{metrics['recall']:<12.4f} {metrics['f1']:<12.4f}")

def plot_confusion_matrices(results, y_test):
    """Plot confusion matrices for all models"""
    print("\nGenerating confusion matrices...")
    
    n_models = len(results)
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.ravel()
    
    for idx, (name, result) in enumerate(results.items()):
        cm = confusion_matrix(y_test, result['predictions'])
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],
                   xticklabels=['Negative', 'Positive'],
                   yticklabels=['Negative', 'Positive'])
        axes[idx].set_title(f'{name}\nConfusion Matrix', fontweight='bold')
        axes[idx].set_ylabel('True Label')
        axes[idx].set_xlabel('Predicted Label')
    
    plt.tight_layout()
    
    # Save and show
    output_path = os.path.join(OUTPUT_DIR, 'confusion_matrices.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"✓ Saved to: {output_path}")
    plt.show()  # Display in Jupyter
    print("✓ Confusion matrices displayed above")

def hyperparameter_tuning(X_train, y_train):
    """Perform hyperparameter tuning on best model"""
    print("\n" + "="*80)
    print("HYPERPARAMETER TUNING - LOGISTIC REGRESSION")
    print("="*80)
    
    param_grid = {
        'C': [0.1, 1, 10],
        'penalty': ['l2'],
        'solver': ['lbfgs', 'liblinear']
    }
    
    print("\nParameter grid:")
    for param, values in param_grid.items():
        print(f"  {param}: {values}")
    
    print("\nPerforming Grid Search with 5-fold cross-validation...")
    
    grid_search = GridSearchCV(
        LogisticRegression(max_iter=1000, random_state=42),
        param_grid,
        cv=5,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    print(f"\n✓ Best parameters: {grid_search.best_params_}")
    print(f"✓ Best cross-validation score: {grid_search.best_score_:.4f}")
    
    return grid_search.best_estimator_

# ============================================================================
# SECTION 6: TOPIC MODELING (Advanced Data Mining)
# ============================================================================

def perform_topic_modeling(df, n_topics=5):
    """Perform topic modeling using LDA"""
    print("\n" + "="*80)
    print("TOPIC MODELING - LATENT DIRICHLET ALLOCATION (LDA)")
    print("="*80)
    
    print(f"\nExtracting {n_topics} topics from reviews...")
    
    # Create document-term matrix
    vectorizer = CountVectorizer(max_features=1000, max_df=0.8, min_df=5)
    doc_term_matrix = vectorizer.fit_transform(df['cleaned_review'])
    
    # Fit LDA model
    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, 
                                    max_iter=20, verbose=0)
    lda.fit(doc_term_matrix)
    
    # Display topics
    feature_names = vectorizer.get_feature_names_out()
    
    print("\nDiscovered Topics:")
    print("-" * 80)
    for topic_idx, topic in enumerate(lda.components_):
        top_words_idx = topic.argsort()[-10:][::-1]
        top_words = [feature_names[i] for i in top_words_idx]
        print(f"\nTopic {topic_idx + 1}: {', '.join(top_words)}")
    
    print("\n✓ Topic modeling complete!")
    
    return lda, vectorizer

# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """Main execution function"""
    print("\n" + "="*80)
    print("IMDB SENTIMENT ANALYSIS - JUPYTER NOTEBOOK VERSION")
    print("="*80)
    print("\nThis project demonstrates:")
    print("  ✓ Exploratory Data Analysis (EDA)")
    print("  ✓ Data Mining Techniques")
    print("  ✓ Text Preprocessing")
    print("  ✓ Feature Engineering")
    print("  ✓ Multiple Machine Learning Models")
    print("  ✓ Model Evaluation & Comparison")
    print("  ✓ Topic Modeling")
    print(f"\n✓ Visualizations will display inline in Jupyter")
    print(f"✓ Also saving to: {OUTPUT_DIR}/")
    
    # Load data
    df = load_data()
    if df is None:
        return
    
    # EDA
    df = perform_eda(df)
    visualize_eda(df)
    
    # Preprocessing (use sample for faster execution - remove sample_size param for full dataset)
    df = preprocess_data(df, sample_size=10000)  # Remove sample_size for full dataset
    
    # Feature engineering
    df = extract_text_features(df)
    
    # Word frequency analysis
    analyze_word_frequency(df)
    
    # Word clouds
    create_word_clouds(df)
    
    # Prepare features (FIXED - now returns only 4 values)
    X_train, X_test, y_train, y_test = prepare_features(df, max_features=5000)
    
    # Train models
    results = train_traditional_models(X_train, X_test, y_train, y_test)
    
    # Visualizations
    visualize_model_comparison(results)
    plot_confusion_matrices(results, y_test)
    
    # Hyperparameter tuning
    best_model = hyperparameter_tuning(X_train, y_train)
    
    # Topic modeling
    perform_topic_modeling(df, n_topics=5)
    
    print("\n" + "="*80)
    print("PROJECT COMPLETE!")
    print("="*80)
    print(f"\n✓ All visualizations displayed above in Jupyter")
    print(f"✓ Files also saved in: {OUTPUT_DIR}/")
    print("\nGenerated files:")
    print("  ✓ eda_visualizations.png")
    print("  ✓ word_frequency.png")
    print("  ✓ wordclouds.png (if wordcloud installed)")
    print("  ✓ model_comparison.png")
    print("  ✓ confusion_matrices.png")
    
    print("\n" + "="*80)

# Run the analysis
if __name__ == "__main__":
    main()
